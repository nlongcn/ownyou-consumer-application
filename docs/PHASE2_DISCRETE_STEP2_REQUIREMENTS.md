# Phase 2: Discrete Step 2 Control - Requirements Document

**Version:** 1.0
**Date:** October 8, 2025
**Status:** Draft - Awaiting Approval

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Current State Analysis](#2-current-state-analysis)
3. [Requirements](#3-requirements)
4. [Technical Specification](#4-technical-specification)
5. [Implementation Plan](#5-implementation-plan)
6. [Testing Strategy](#6-testing-strategy)
7. [Risks & Mitigations](#7-risks--mitigations)

---

## 1. Executive Summary

### Objective
Add discrete control over Step 2 (Email Summarization) to enable users to:
- Re-run summarization with different EMAIL_MODEL without re-downloading
- Test email models independently of classification models
- Resume from raw email CSV checkpoint
- Separate the fast summarization step from the expensive classification step

### Current Limitation
Currently, summarization happens bundled with IAB classification:
```bash
# This does BOTH summarization AND classification
python -m src.email_parser.main --iab-csv raw.csv --iab-output profile.json
```

### Proposed Solution
Add `--summarize-only` flag to create a discrete Step 2:
```bash
# Step 2: Summarization only
python -m src.email_parser.main \
  --summarize-only \
  --input-csv data/raw_emails.csv \
  --output data/summaries.csv \
  --email-model google:gemini-2.0-flash-exp
```

---

## 2. Current State Analysis

### 2.1 Existing Three-Stage Flow

**Current Implementation:**

```
Step 1: Email Download (✅ Already discrete)
--pull-only --provider gmail --max-emails 100 --output raw.csv
↓
Step 2+3: Summarization + Classification (❌ Bundled together)
--iab-csv raw.csv --iab-output profile.json
```

**Problem:**
- Cannot re-run summarization without re-running classification
- Cannot test different email models without incurring classification costs
- Cannot preserve summaries as a checkpoint between download and classification

### 2.2 Code Flow Analysis

**Current bundled flow in `main.py`:**

1. `--iab-csv raw.csv` is provided
2. Raw emails loaded from CSV
3. `process_emails_with_llm()` called → **Summarization happens here**
4. Summaries are NOT saved to CSV
5. `run_iab_workflow()` called → **Classification happens here**
6. Profile JSON saved

**Key Finding:**
Summaries are created but **never persisted** between steps. They're ephemeral intermediate data.

### 2.3 What Step 2 Must Do

1. Read raw emails CSV (columns: `email_id`, `subject`, `sender`, `date`, `body`, `labels`)
2. Process with EMAIL_MODEL to create summaries
3. Save summaries CSV with columns:
   - `email_id` (from input)
   - `subject` (from input)
   - `sender` (from input)
   - `date` (from input)
   - `summary` (generated by LLM)
   - `category` (generated by LLM)
   - `sentiment` (generated by LLM)
   - `products_mentioned` (generated by LLM)
   - `links` (generated by LLM)
   - All existing processed email fields

**Critical:** Step 3 must be able to consume this summaries CSV.

---

## 3. Requirements

### 3.1 Functional Requirements

#### FR-1: Discrete Summarization Mode
- **ID:** FR-1
- **Priority:** P0 (Critical)
- **Description:** Add `--summarize-only` flag to run email summarization without classification
- **Acceptance Criteria:**
  - Flag exists in argparse configuration
  - When enabled, only summarization logic runs (no classification)
  - Outputs summaries CSV to `--output` path
  - Uses EMAIL_MODEL from environment or `--email-model` arg
  - Can process from raw emails CSV via `--input-csv`

#### FR-2: Input CSV Compatibility
- **ID:** FR-2
- **Priority:** P0 (Critical)
- **Description:** `--summarize-only` must accept raw emails CSV from Step 1
- **Acceptance Criteria:**
  - Reads CSV with raw email fields (email_id, subject, sender, date, body, labels)
  - Handles both Gmail and Outlook CSV formats
  - Validates required columns exist
  - Reports clear error if CSV format invalid

#### FR-3: Summaries CSV Output
- **ID:** FR-3
- **Priority:** P0 (Critical)
- **Description:** Generate summaries CSV that Step 3 can consume
- **Acceptance Criteria:**
  - CSV contains all ProcessedEmail fields
  - Includes: email_id, summary, category, sentiment, products_mentioned, links
  - Preserves raw email metadata (subject, sender, date)
  - Compatible with existing `--iab-csv` input format

#### FR-4: Step 3 Compatibility
- **ID:** FR-4
- **Priority:** P0 (Critical)
- **Description:** Step 3 must accept summaries CSV from Step 2
- **Acceptance Criteria:**
  - `--iab-csv summaries.csv` works with summaries from `--summarize-only`
  - No changes needed to existing IAB workflow
  - Classification uses summaries (not raw body text)
  - Backwards compatible with existing CSV files

#### FR-5: Model Selection Override
- **ID:** FR-5
- **Priority:** P1 (High)
- **Description:** Allow per-step model override via CLI args
- **Acceptance Criteria:**
  - `--email-model google:gemini-2.0-flash-exp` overrides .env EMAIL_MODEL
  - `--taxonomy-model openai:gpt-4o-mini` overrides .env TAXONOMY_MODEL
  - Defaults to .env if not specified
  - Model format validated (provider:model)

#### FR-6: Progress Tracking
- **ID:** FR-6
- **Priority:** P2 (Medium)
- **Description:** Log progress during summarization
- **Acceptance Criteria:**
  - Logs "Processing email X/Y" during summarization
  - Reports total time taken
  - Logs model used (provider:model)
  - Compatible with dashboard log streaming

### 3.2 Non-Functional Requirements

#### NFR-1: Backward Compatibility
- **ID:** NFR-1
- **Priority:** P0 (Critical)
- **Description:** Existing workflows must continue to work unchanged
- **Acceptance Criteria:**
  - Current `--iab-csv raw.csv` command still works (summarization + classification)
  - Existing CSV files can be processed
  - No breaking changes to public API
  - All existing tests pass

#### NFR-2: Performance
- **ID:** NFR-2
- **Priority:** P1 (High)
- **Description:** Summarization-only mode should be fast
- **Acceptance Criteria:**
  - No unnecessary I/O operations
  - Concurrent processing enabled (if configured)
  - Memory usage reasonable for 1000+ email batches

#### NFR-3: Error Handling
- **ID:** NFR-3
- **Priority:** P1 (High)
- **Description:** Clear error messages for common issues
- **Acceptance Criteria:**
  - Invalid CSV format → descriptive error
  - Missing EMAIL_MODEL → clear instructions
  - Invalid model format → example provided
  - LLM API errors → retry with backoff

---

## 4. Technical Specification

### 4.1 New CLI Flags

```python
# In create_argument_parser()

parser.add_argument(
    '--summarize-only',
    action='store_true',
    help='Summarize emails using EMAIL_MODEL without running IAB classification'
)

parser.add_argument(
    '--input-csv',
    type=str,
    help='Input CSV file (raw emails for summarization, summaries for classification)'
)

parser.add_argument(
    '--email-model',
    type=str,
    help='Override EMAIL_MODEL from .env (format: provider:model, e.g., google:gemini-2.0-flash-exp)'
)

parser.add_argument(
    '--taxonomy-model',
    type=str,
    help='Override TAXONOMY_MODEL from .env (format: provider:model, e.g., openai:gpt-4o-mini)'
)
```

### 4.2 Execution Flow

**Step 2 Flow (`--summarize-only`):**

```python
def main():
    args = parser.parse_args()

    if args.summarize_only:
        # Step 2: Summarization only

        # 1. Validate inputs
        if not args.input_csv:
            raise ValueError("--summarize-only requires --input-csv")

        if not os.path.exists(args.input_csv):
            raise FileNotFoundError(f"Input CSV not found: {args.input_csv}")

        # 2. Load raw emails from CSV
        raw_emails = load_raw_emails_from_csv(args.input_csv)

        # 3. Get email model (CLI arg overrides .env)
        email_model = args.email_model or config.llm.email_model
        if not email_model:
            raise ValueError("EMAIL_MODEL must be specified in .env or via --email-model")

        # 4. Process emails with LLM (summarization)
        parser = EmailParser(config)
        processed_emails = parser.process_emails_with_llm(
            raw_emails,
            instruction=args.instruction
        )

        # 5. Save summaries to CSV
        output_path = args.output or 'emails_summaries.csv'
        save_processed_emails_to_csv(processed_emails, output_path)

        logger.info(f"✅ Summarization complete: {len(processed_emails)} emails → {output_path}")
        return
```

**Step 3 Flow (`--iab-csv`):**

```python
# Existing code, minimal changes needed
if args.iab_csv:
    # Load emails from CSV (could be raw OR summaries)
    emails_data = load_emails_from_csv(args.iab_csv)

    # If CSV has 'summary' column, use summaries (Step 2 output)
    # If CSV has 'body' column, summarize first (legacy raw CSV)

    if 'summary' in emails_data.columns:
        # Step 2 was already done - use existing summaries
        processed_emails = parse_summaries_from_csv(emails_data)
    else:
        # Legacy raw CSV - do summarization now (backward compat)
        raw_emails = parse_raw_emails_from_csv(emails_data)
        processed_emails = parser.process_emails_with_llm(raw_emails)

    # Continue with IAB classification workflow
    run_iab_workflow(processed_emails, ...)
```

### 4.3 CSV Formats

**Step 1 Output (Raw Emails CSV):**
```csv
email_id,subject,sender,date,body,labels
msg123,"Hello","sender@example.com","2025-10-01","Email body text here","INBOX,IMPORTANT"
```

**Step 2 Output (Summaries CSV):**
```csv
email_id,subject,sender,date,summary,category,sentiment,products_mentioned,links
msg123,"Hello","sender@example.com","2025-10-01","Email summary","Personal","positive","[]","[]"
```

**Step 3 Input:**
Either Step 1 CSV (raw) OR Step 2 CSV (summaries)

### 4.4 Helper Functions Needed

```python
def load_raw_emails_from_csv(csv_path: str) -> List[RawEmail]:
    """
    Load raw emails from Step 1 CSV.

    Required columns: email_id, subject, sender, date, body
    Optional columns: labels, provider
    """
    pass

def save_processed_emails_to_csv(emails: List[ProcessedEmail], output_path: str):
    """
    Save processed emails (with summaries) to CSV.

    Output columns: email_id, subject, sender, date, summary,
                   category, sentiment, products_mentioned, links
    """
    pass

def parse_summaries_from_csv(df: pd.DataFrame) -> List[ProcessedEmail]:
    """
    Parse summaries CSV (Step 2 output) into ProcessedEmail objects.

    Used by Step 3 to avoid re-summarization.
    """
    pass
```

### 4.5 Configuration Priority

For model selection:

1. **CLI argument** (highest priority)
   - `--email-model google:gemini-2.0-flash-exp`
2. **.env file** (fallback)
   - `EMAIL_MODEL=google:gemini-2.0-flash-exp`
3. **Error** (no default)
   - Fail with clear message if neither provided

---

## 5. Implementation Plan

### 5.1 Implementation Steps

**Step 1: Add CLI Flags (Low Risk)**
- Add `--summarize-only`, `--input-csv`, `--email-model`, `--taxonomy-model` to argparse
- Update help text and examples
- Commit: "Add CLI flags for discrete Step 2 control"

**Step 2: Implement Helper Functions (Medium Risk)**
- `load_raw_emails_from_csv()`
- `save_processed_emails_to_csv()`
- `parse_summaries_from_csv()`
- Unit tests for each function
- Commit: "Add CSV I/O helpers for discrete steps"

**Step 3: Implement --summarize-only Logic (Medium Risk)**
- Add execution branch for `args.summarize_only`
- Validate inputs
- Call existing `process_emails_with_llm()`
- Save summaries CSV
- Commit: "Implement --summarize-only mode"

**Step 4: Update --iab-csv Logic (Medium Risk)**
- Detect if input CSV has summaries or raw emails
- Skip summarization if summaries already exist
- Maintain backward compatibility
- Commit: "Add summaries CSV support to IAB workflow"

**Step 5: Add Model Override Logic (Low Risk)**
- Parse `--email-model` and `--taxonomy-model`
- Override config values if provided
- Validate format (provider:model)
- Commit: "Add per-step model override support"

**Step 6: Testing (High Effort)**
- Unit tests for all new functions
- Integration test: Step 1 → Step 2 → Step 3 pipeline
- Backward compatibility test: existing CSV files
- Manual testing with dashboard
- Commit: "Add comprehensive tests for discrete steps"

**Step 7: Dashboard Integration**
- Update backend API to call new CLI flags
- Add discrete step endpoints
- Update frontend UI
- Commit: "Integrate discrete steps with dashboard"

### 5.2 File Changes

**Modified Files:**
- `src/email_parser/main.py` (CLI logic, ~150 lines added)
- `src/email_parser/models/email.py` (possibly add helpers)
- `tests/test_discrete_steps.py` (new test file)

**No Changes Needed:**
- `src/email_parser/workflow/` (existing IAB workflow unchanged)
- `src/email_parser/llm_clients/` (no changes to LLM clients)
- `src/email_parser/providers/` (no changes to email providers)

### 5.3 Timeline Estimate

- **Step 1-2:** 1 hour (CLI flags + helpers)
- **Step 3-4:** 2 hours (Core logic + IAB integration)
- **Step 5:** 30 minutes (Model override)
- **Step 6:** 2 hours (Testing)
- **Step 7:** 3 hours (Dashboard integration)

**Total:** ~8-9 hours for complete implementation

---

## 6. Testing Strategy

### 6.1 Unit Tests

```python
def test_load_raw_emails_from_csv():
    """Test loading raw emails from Step 1 CSV."""
    csv_path = 'tests/fixtures/raw_emails.csv'
    emails = load_raw_emails_from_csv(csv_path)
    assert len(emails) == 10
    assert emails[0].email_id == 'msg123'
    assert emails[0].body is not None

def test_save_processed_emails_to_csv():
    """Test saving summaries to CSV."""
    processed = [ProcessedEmail(...)]
    save_processed_emails_to_csv(processed, '/tmp/summaries.csv')
    assert os.path.exists('/tmp/summaries.csv')
    df = pd.read_csv('/tmp/summaries.csv')
    assert 'summary' in df.columns

def test_parse_summaries_from_csv():
    """Test parsing summaries CSV into ProcessedEmail objects."""
    df = pd.read_csv('tests/fixtures/summaries.csv')
    processed = parse_summaries_from_csv(df)
    assert len(processed) == 10
    assert processed[0].summary is not None
```

### 6.2 Integration Tests

```bash
# Test 1: Full 3-step pipeline
python -m src.email_parser.main --pull-only --provider gmail --max-emails 10 --output /tmp/raw.csv
python -m src.email_parser.main --summarize-only --input-csv /tmp/raw.csv --output /tmp/summaries.csv
python -m src.email_parser.main --iab-csv /tmp/summaries.csv --iab-output /tmp/profile.json --user-id test

# Test 2: Backward compatibility
python -m src.email_parser.main --iab-csv /tmp/raw.csv --iab-output /tmp/profile2.json --user-id test

# Test 3: Model override
python -m src.email_parser.main --summarize-only --input-csv /tmp/raw.csv --output /tmp/summaries2.csv --email-model google:gemini-2.0-flash-exp
```

### 6.3 Manual Testing Checklist

- [ ] Step 1 → Step 2 → Step 3 pipeline completes successfully
- [ ] Re-running Step 2 with different model works
- [ ] Re-running Step 3 with different model works
- [ ] Existing raw CSV files still work with `--iab-csv`
- [ ] Dashboard can trigger all 3 discrete steps
- [ ] Checkpoint files are saved correctly
- [ ] Cost tracking works per step
- [ ] Logs are clear and informative

---

## 7. Risks & Mitigations

### 7.1 Risk: Breaking Existing Workflows

**Likelihood:** Medium
**Impact:** High
**Mitigation:**
- Maintain strict backward compatibility
- Existing `--iab-csv` behavior unchanged (auto-detects raw vs summaries)
- Comprehensive regression testing
- Phased rollout (test with one user first)

### 7.2 Risk: CSV Format Incompatibility

**Likelihood:** Medium
**Impact:** Medium
**Mitigation:**
- Validate CSV columns before processing
- Clear error messages with expected format
- Support both raw and summaries formats in Step 3
- Document CSV schemas explicitly

### 7.3 Risk: Model Override Conflicts

**Likelihood:** Low
**Impact:** Low
**Mitigation:**
- Clear priority: CLI arg > .env > error
- Validate model format before execution
- Log which model is actually being used
- Error early if model not found

### 7.4 Risk: Performance Regression

**Likelihood:** Low
**Impact:** Low
**Mitigation:**
- Minimize additional I/O operations
- Use streaming for large CSVs
- Profile before/after with 1000 email batch
- Optimize if >10% performance degradation

---

## 8. Success Criteria

✅ **Implementation Complete When:**

1. All 3 steps can be run independently from CLI
2. Summaries CSV can be generated and saved
3. Step 3 accepts summaries CSV as input
4. Backward compatibility maintained (all existing tests pass)
5. Dashboard API can trigger discrete steps
6. Integration tests pass for full pipeline
7. Documentation updated (CLI help, CLAUDE.md)

✅ **User Value Delivered:**

- Users can re-run Step 2 without re-downloading emails
- Users can test different email models independently
- Users can resume from Step 2 checkpoint
- Users save money by skipping unnecessary LLM calls
- System is more resilient to failures

---

## Appendix A: Example Commands

### Full Pipeline (3 Discrete Steps)

```bash
# Step 1: Download emails
python -m src.email_parser.main \
  --pull-only \
  --provider gmail outlook \
  --max-emails 100 \
  --output data/user_123_raw_20251008.csv

# Step 2: Summarize emails
python -m src.email_parser.main \
  --summarize-only \
  --input-csv data/user_123_raw_20251008.csv \
  --output data/user_123_summaries_20251008.csv \
  --email-model google:gemini-2.0-flash-exp

# Step 3: IAB classification
python -m src.email_parser.main \
  --iab-csv data/user_123_summaries_20251008.csv \
  --iab-output data/user_123_profile_20251008.json \
  --user-id user_123 \
  --taxonomy-model openai:gpt-4o-mini
```

### Re-run Step 2 with Different Model

```bash
# Original summarization with Gemini
python -m src.email_parser.main \
  --summarize-only \
  --input-csv data/raw.csv \
  --output data/summaries_gemini.csv \
  --email-model google:gemini-2.0-flash-exp

# Re-run with GPT-4o-mini
python -m src.email_parser.main \
  --summarize-only \
  --input-csv data/raw.csv \
  --output data/summaries_gpt4o.csv \
  --email-model openai:gpt-4o-mini

# Compare results without re-downloading or re-classifying
```

### Backward Compatible (Existing Workflow)

```bash
# This still works exactly as before (summarization + classification)
python -m src.email_parser.main \
  --iab-csv data/raw.csv \
  --iab-output data/profile.json \
  --user-id user_123
```

---

**End of Requirements Document**

**Next Steps:**
1. Review and approve this requirements document
2. Implement Step 1-6 from Implementation Plan
3. Test thoroughly with integration tests
4. Update dashboard API and frontend
5. Deploy and validate with real user workflow
